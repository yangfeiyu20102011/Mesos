在传统分布式系统中，需要一个Coordinator调度器来指挥多个Worker来执行任务。那么，当调度器分配任务时就需要与具体的机器直接去打交道，机器资源被静态划分，其灵活性也大大降低。Mesos充当调度器和机器之间的中间人员，调度器只需要与Mesos去沟通而不必直接和物理机器打交道。而且上层拥有不同分布式系统时不会影响Mesos对于资源的掌控。Mesos提供了一个通用功能集（故障检测、分布式任务、任务启动、任务监控、结束任务、清理任务等），上层系统不用重复去实现这样一套逻辑。
在《Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center》这篇论文中，给出了Mesos的设计架构和运作方式。
Mesos的两级调度结构设计意图在于保持一套尽可能小巧的公用接口实现跨框架的资源共享，将任务的控制和执行交给Framework去处理，能让Framework针对具体情况如数据局部性、错误处理等实施多种应对方案，并持续独立地去改进这些方案。Mesos提供一套低层次的接口，这也使得Mesos能够一直延续其简单风格，保持小巧和灵活特性，让复杂功能在更高层级去独立地实现。

Slave职责，负责接收并执行master命令，管理节点上的tasks。Slave会将自己的资源情况发送给master，任务分配后，slave会将任务放到包含固定资源的LXC中运行，达到资源隔离的效果。
Scheduler职责，根据作业输入情况，将其拆分成若干任务，并向Master提出资源申请，并监控任务运行状态，有任务失败则重新申请资源。
Executor，在Slave节点上，负责执行对应Framework的具体任务。Mesos为了向各种不同框架提供统一的executor编写方式，内部用c++实现了一个MesosExecutorDriver，上层的Framework可通过该驱动器接口告诉Mesos如何去执行tasks。
Framework大体可以分为两类，一类是对资源需求不固定的Hadoop、Spark等，一类是对资源需求固定的MPI。
Master节点是整个集群的中枢，它责管理和分配整个Mesos集群的计算资源，调度上层Framework提交的任务，管理和分发所有任务的状态。
Master地位非常重要，为了保证它的正常运作。设计者采用两种方式来保证其可靠性，一是从master自身设计出发，能够从slave和framework scheduler获得的信息中重建当前状态。二是采用了主备冗余模式来部署多个Master节点，使用ZooKeeper来进行Leader选举，其他Master节点则作为Follower处于备用状态，并实时监控当前状态，在Leader异常时，从中重新产生洗的Leader来保证服务可靠。
在部署Mesos时，要首先部署好Zookeeper。其选举机制是，首先指定一个Znode，然后使用SEQUENCE和EPHEMERAL标志为每一个要竞选的client创建一个Znode，当为某个Znode节点设置SEQUENCE标志时，ZooKeeper会在其名称后追加一个自增序号，这个序列号要比最近一次在同一个目录下加入的znode的序列号大。具体做法首先需要在ZooKeeper中创建一个父Znode，然后指定SEQUENCE｜EPHEMERAL标志为每一个Mesos master节点创建一个子的Znode，并在名称之后追加自增的序列号。当为某个Znode节点设置EPHEMERAL标志时，当这个节点所属的客户端和ZooKeeper之间的seesion断开之后，这个节点将会被ZooKeeper自动删除。 ZooKeeper的选举机制就是在父Znode下的子Znode中选出序列号最小的作为Leader。同时，ZooKeeper提供了监视（watch）的机制，其他的非Master节点会不断监视当前的Leader所对应的Znode，如果它被删除，则触发新一轮的选举。选举方式大体分为两种，第一种是所有的非Leader client监视当前Leader对应的Znode（也就是序列号最小的Znode），当它被ZooKeeper删除的时候，所有监视它的客户端会立即收到通知，然后调用API查询所有在父目录下的子节点，如果它对应的序列号是最小的，则这个client会成为新的Leader对外提供服务，然后其他客户端继续监视这个新Leader对应的Znode。这种方式会触发“羊群效应”，特别是在选举集群比较大的时候，在新一轮选举开始时，所有的客户端都会调用ZooKeeper的API查询所有的子Znode来决定谁是下一个Leader，这个时候情况就更为明显。 第二种是，为了避免“羊群效应”，建议每一个非Leader的client监视集群中对应的比自己节点序小一号的节点，只有当某个client所设置的watch被触发时，它才进行Leader选举操作：查询所有的子节点，看自己是不是序号最小的，如果是，那么它将成为新的Leader，如果不是，继续监视。此 Leader选举操作的速度是很快的，因为每一次选举几乎只涉及单个client的操作。 
Mesos高可用实现细节，主要通过contender和detector两个模块来实现

Contender模块用来进行Leader选举，Detector模块用来感知当前的master是谁，它主要利用ZooKeeper中的watcher的机制来监控选举的Group的变化。ZooKeeper提供了getChildren()应用程序接口，此接口可以用来监控一个目录下子节点的变化，如果一个新子节点加入或者原来的节点被删除，那么这个函数调用会立即返回当前目录下的所有节点，然后Detector模块可以挑选序号最小的作为master节点。
当原master节点失效，新的master节点产生后，会首先从Replicated log中恢复之前的状态。包括Mesos集群的配置信息，Maintenance信息，所有注册的Agent信息等。新的master会为agents设立一个重新注册的超时时间，如果agent没有在相应时间内完成对新master的重新注册，则会从Replicated log中删除，并且不能以原先身份注册到新master，该agent之前的运行任务也将全部丢失。为了避免某些情况下agent的大量删除而引起过多重要任务的丢失，master提供一个flag配置recovery_slave_removal_limit，当删除的agent百分比超过该标志后master节点会失效来触发新一轮的leader选举。在agent重新注册时，会上报它们的checkpointed资源，运行的executors和tasks信息以及所有tasks完成的Framework信息，以便新的master恢复之前的状态。
Master通过两种机制来监控已经注册的Mesos Agents健康状况和可用性。一是master会持久化和每个Agent之间的TCP的链接，如果某个Agent服务宕机，那么master会第一时间感知到，然后将agent设为休眠状态，agent上的资源也将不会offer给上层Framework，并触发rescind offer将已offer资源撤销，触发rescind inverse offer将inverse offer撤销。二是不断向agent发送ping消息，如果未在设定时间内收到回复且到达一定次数，则将该agent从master中删除并且遍历agent上运行的所有任务，向相应Framework发送TASK_LOST状态更新删除指定任务，随后删除该agent上的所有executor并触发rescind offer和rescind inverse offer，最后将agent从Replicated log中删除。
Master对Framework的健康检查方案与对Agents的健康检查方案类似。


从社区的活动情况上看，Mesos更专注于集群资源的管理，开始引入更多Kubernetes的概念来支持Kubernetes API，与Kubernetes的竞争比较明显。Mesos目前对计算集群支持的比较友好，而Kubernetes对虚拟机，尤其是云端支持的更友好。
Mesos 更多的是从经济学角度出发来提高整个集群的资源利用率，拿它跟 YARN、Google Borg 作比较更合适；而 Docker Swarm 专注于 Docker 的集群管理，拿它跟 Kubernetes 比较可能更合适。想搭建一套集群生产环境，从稳定性和可扩展性来看，建议选择Mesos，如果是仅想运行Docker容器，从易用性角度来看，建议采用Docker Swarm。
Mesos和Swarm没有明显的竞争关系，至少在目前是这样。相反，它们有很多的交集，甚至可以很好地集成。有部分团队也开始研究将Swarm运行在Mesos之上，原因主要有：各大云厂商都开始拥抱Mesos，Swarm在其之上能进一步扩大其应用空间；Swarm能充分利用Mesos的弹性资源分配机制，能和其他Framework共享资源，进一步提高资源利用率；填补了Swarm对多租户支持的空白，借助Mesos能够更轻松地实现多租户。
